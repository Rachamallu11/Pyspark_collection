{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054b6030-d899-47fe-9cf2-2c34622bb866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1| satya|\n|  2|  rana|\n|  3|rochar|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# create the DataFrame in pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n",
    "\n",
    "#import data\n",
    "Data = [(1,\"satya\"),(2,\"rana\"),(3,\"rochar\")]\n",
    "\n",
    "schema = [\"id\",\"name\"]\n",
    "\n",
    "#create the dataframe\n",
    "df = spark.createDataFrame(Data,schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71582569-267f-4cdf-9f21-f2c26aa7efe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1| leela|\n|  2|sekhar|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#creating DataFrame using Dictionary\n",
    "\n",
    "data_dict = [{\"id\":1,\"name\":\"leela\"},{\"id\":2,\"name\":\"sekhar\"}]\n",
    "data_dict_df = spark.createDataFrame(data_dict).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06795a3a-9d0e-448c-a7f6-0be783b8b65b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n| id|name|age|\n+---+----+---+\n+---+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#creating the DataFrame with empty fields just schema is provided\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "#define schema\n",
    "schema = StructType([StructField(\"id\",IntegerType(),True)\n",
    "                     ,StructField(\"name\", StringType(),True)\n",
    "                     ,StructField(\"age\",IntegerType(),True)])\n",
    "\n",
    "df_schema = spark.createDataFrame([],schema)\n",
    "df_schema.show()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db581c97-d3dc-4e27-9455-26ae028ebf04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2089852340131066>, line 5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#create the data from structured data(like: json,csv,parquet formats)\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#1.csv\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m df_csv \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/path/to/file.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,header \u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,inferSchema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m      6\u001B[0m df_csv\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:837\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n",
       "\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\u001B[1;32m    836\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 837\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(path)))\n",
       "\u001B[1;32m    839\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_remote_only():\n",
       "\u001B[1;32m    840\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrdd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RDD  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/file.csv. SQLSTATE: 42K03"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/file.csv. SQLSTATE: 42K03"
       },
       "metadata": {
        "errorSummary": "[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/file.csv. SQLSTATE: 42K03"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "PATH_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": "42K03",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-2089852340131066>, line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#create the data from structured data(like: json,csv,parquet formats)\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#1.csv\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m df_csv \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/path/to/file.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,header \u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,inferSchema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m df_csv\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:837\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    836\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 837\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(path)))\n\u001B[1;32m    839\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_remote_only():\n\u001B[1;32m    840\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrdd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RDD  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/file.csv. SQLSTATE: 42K03"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create the data from structured data(like: json,csv,parquet formats)\n",
    "\n",
    "#1.csv\n",
    "\n",
    "df_csv = spark.read.csv(\"/path/to/file.csv\",header =True,inferSchema = True)  #here we need to give the path\n",
    "df_csv.show()\n",
    "\n",
    "#2.json\n",
    "\n",
    "df_json = spark.read.json(\"/path/to/file.json\",header =True, inferSchema = True)\n",
    "df_json.show()\n",
    "\n",
    "#3.parquet\n",
    "\n",
    "df_parquet = spark.read.parquet(\"/path/to/file.parquet\",header = True, inferSchema = True)\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d924d2ab-ccfd-4f3a-8562-54488d5aa4b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2089852340131067>, line 15\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m employ_schema \u001B[38;5;241m=\u001B[39m StructType([StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m,IntegerType(),\u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[1;32m      6\u001B[0m                      StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m,StringType(),\u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[1;32m      7\u001B[0m                      StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,IntegerType(),\u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     11\u001B[0m                      StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPhone\u001B[39m\u001B[38;5;124m\"\u001B[39m,IntegerType(),\u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[1;32m     12\u001B[0m                      StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAddress\u001B[39m\u001B[38;5;124m\"\u001B[39m,StringType(),\u001B[38;5;28;01mTrue\u001B[39;00m)])\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#load the dataframe with the defined schema\u001B[39;00m\n",
       "\u001B[0;32m---> 15\u001B[0m df_schema \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Filestore/tables/employee.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m, header \u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,schema \u001B[38;5;241m=\u001B[39m employ_schema)\n",
       "\u001B[1;32m     17\u001B[0m df_schema\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\u001B[1;32m     18\u001B[0m df_schema\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:312\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    310\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
       "\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mload(path))\n",
       "\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/Filestore/tables/employee.csv. SQLSTATE: 42K03"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[PATH_NOT_FOUND] Path does not exist: dbfs:/Filestore/tables/employee.csv. SQLSTATE: 42K03"
       },
       "metadata": {
        "errorSummary": "[PATH_NOT_FOUND] Path does not exist: dbfs:/Filestore/tables/employee.csv. SQLSTATE: 42K03"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "PATH_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": "42K03",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-2089852340131067>, line 15\u001B[0m\n\u001B[1;32m      5\u001B[0m employ_schema \u001B[38;5;241m=\u001B[39m StructType([StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m,IntegerType(),\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      6\u001B[0m                      StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m,StringType(),\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      7\u001B[0m                      StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,IntegerType(),\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m                      StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPhone\u001B[39m\u001B[38;5;124m\"\u001B[39m,IntegerType(),\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m     12\u001B[0m                      StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAddress\u001B[39m\u001B[38;5;124m\"\u001B[39m,StringType(),\u001B[38;5;28;01mTrue\u001B[39;00m)])\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#load the dataframe with the defined schema\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m df_schema \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Filestore/tables/employee.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m, header \u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,schema \u001B[38;5;241m=\u001B[39m employ_schema)\n\u001B[1;32m     17\u001B[0m df_schema\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m     18\u001B[0m df_schema\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:312\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mload(path))\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/Filestore/tables/employee.csv. SQLSTATE: 42K03"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Define schema programmatically with StructType\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "employ_schema = StructType([StructField(\"id\",IntegerType(),True),\n",
    "                     StructField(\"name\",StringType(),True),\n",
    "                     StructField(\"age\",IntegerType(),True),\n",
    "                     StructField(\"Salary\",DoubleType(),True),\n",
    "                     StructField(\"Joining_Date\",StringType(),True),  #keeping as string for data issue\n",
    "                     StructField(\"Email\",StringType(),True),\n",
    "                     StructField(\"Phone\",IntegerType(),True),\n",
    "                     StructField(\"Address\",StringType(),True)])\n",
    "\n",
    "#load the dataframe with the defined schema\n",
    "df_schema = spark.read.load(\"/Filestore/tables/employee.csv\",format = \"csv\", header =True,schema = employ_schema)\n",
    "\n",
    "df_schema.printSchema()\n",
    "df_schema.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82020b5-68c0-4a90-8040-255cdd5a1dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+--------------------+\n| id|   name|   department|              skills|\n+---+-------+-------------+--------------------+\n|  1|florida|    marketing|Social Media mark...|\n|  2| George|        Sales|     Lead Generation|\n|  3| Hannah|  Engineering|    Machine Learning|\n|  4|    Ian|Customer Care|      CRM Management|\n|  5|  Julia|      Finance|       Risk Analysis|\n+---+-------+-------------+--------------------+\n\n+---+-------+-------------+--------------------+-------+----------+---------+\n| id|   name|   department|              skills|skills1|   skills2|  skills3|\n+---+-------+-------------+--------------------+-------+----------+---------+\n|  1|florida|    marketing|Social Media mark...| Social|     Media|marketing|\n|  2| George|        Sales|     Lead Generation|   Lead|Generation|     NULL|\n|  3| Hannah|  Engineering|    Machine Learning|Machine|  Learning|     NULL|\n|  4|    Ian|Customer Care|      CRM Management|    CRM|Management|     NULL|\n|  5|  Julia|      Finance|       Risk Analysis|   Risk|  Analysis|     NULL|\n+---+-------+-------------+--------------------+-------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#1.Split() and explode() functions \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, array_contains, col, size\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Splitexplode\").getOrCreate()\n",
    "\n",
    "data = [(1, \"florida\",\"marketing\",\"Social Media marketing\"),\n",
    "                (2, \"George\", \"Sales\", \"Lead Generation\"), \n",
    "                (3, \"Hannah\", \"Engineering\",\"Machine Learning\"),\n",
    "                 (4, \"Ian\", \"Customer Care\",\"CRM Management\"),\n",
    "                  (5, \"Julia\", \"Finance\", \"Risk Analysis\") ]\n",
    "\n",
    "column = [\"id\", \"name\", \"department\", \"skills\"]\n",
    "\n",
    "df = spark.createDataFrame(data, column)\n",
    "df.show()\n",
    "\n",
    "split_df = df.withColumn(\"skills1\", split(col(\"skills\"),\" \")[0])\\\n",
    "             .withColumn(\"skills2\", split(col(\"skills\"),\" \")[1])\\\n",
    "             .withColumn(\"skills3\", split(col(\"skills\"),\" \")[2])  \n",
    "\n",
    "df.drop(\"skills\")\n",
    "split_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d3b293-3993-44b9-86d5-a28ce84bf73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+--------------------+--------------------+-----------+\n| id|   name|   department|              skills|        Skills_Array|First_Skill|\n+---+-------+-------------+--------------------+--------------------+-----------+\n|  1|florida|    marketing|Social Media mark...|[Social, Media, m...|     Social|\n|  2| George|        Sales|     Lead Generation|  [Lead, Generation]|       Lead|\n|  3| Hannah|  Engineering|    Machine Learning| [Machine, Learning]|    Machine|\n|  4|    Ian|Customer Care|      CRM Management|   [CRM, Management]|        CRM|\n|  5|  Julia|      Finance|       Risk Analysis|    [Risk, Analysis]|       Risk|\n+---+-------+-------------+--------------------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#2.Select the first skill from the \"Skills_Array\":\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, array_contains, col, size\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Splitexplode\").getOrCreate()\n",
    "\n",
    "data = [(1, \"florida\",\"marketing\",\"Social Media marketing\"),\n",
    "                (2, \"George\", \"Sales\", \"Lead Generation\"), \n",
    "                (3, \"Hannah\", \"Engineering\",\"Machine Learning\"),\n",
    "                 (4, \"Ian\", \"Customer Care\",\"CRM Management\"),\n",
    "                  (5, \"Julia\", \"Finance\", \"Risk Analysis\") ]\n",
    "\n",
    "column = [\"id\", \"name\", \"department\", \"skills\"]\n",
    "df = spark.createDataFrame(data= data, schema = column)\n",
    "split_1_df = df.withColumn(\"Skills_Array\", split(col(\"skills\"),\" \"))\n",
    "\n",
    "df2 = split_1_df.withColumn(\"First_Skill\", col(\"Skills_Array\").getItem(0))\n",
    "df2.drop(\"skills\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39eb786-4ae9-431e-bfb3-4c12a19708dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+-------------+\n| id|   name|   department|explode_skill|\n+---+-------+-------------+-------------+\n|  1|florida|    marketing|       Social|\n|  1|florida|    marketing|        Media|\n|  1|florida|    marketing|    marketing|\n|  2| George|        Sales|         Lead|\n|  2| George|        Sales|   Generation|\n|  3| Hannah|  Engineering|      Machine|\n|  3| Hannah|  Engineering|     Learning|\n|  4|    Ian|Customer Care|          CRM|\n|  4|    Ian|Customer Care|   Management|\n|  5|  Julia|      Finance|         Risk|\n|  5|  Julia|      Finance|     Analysis|\n+---+-------+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#3..explode() : Use the explode function to transform array elements into individual rows:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, array_contains, col, size\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Splitexplode\").getOrCreate()\n",
    "\n",
    "data = [(1, \"florida\",\"marketing\",\"Social Media marketing\"),\n",
    "                (2, \"George\", \"Sales\", \"Lead Generation\"), \n",
    "                (3, \"Hannah\", \"Engineering\",\"Machine Learning\"),\n",
    "                 (4, \"Ian\", \"Customer Care\",\"CRM Management\"),\n",
    "                  (5, \"Julia\", \"Finance\", \"Risk Analysis\") ]\n",
    "\n",
    "column = [\"id\", \"name\", \"department\", \"skills\"]\n",
    "df = spark.createDataFrame(data= data, schema = column)\n",
    "split_1_df = df.withColumn(\"Skills_Array\", split(col(\"skills\"),\" \"))\n",
    "\n",
    "explode_df = split_1_df.withColumn(\"explode_skill\", explode(col(\"Skills_Array\"))).select(\"id\",\"name\",\"department\",\"explode_skill\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc016633-d944-4f77-891c-edc22e276269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#JOINS\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data Frame1\").getOrCreate()\n",
    "\n",
    "emp = [(1, \"Smith\", -1, \"2018\", \"10\", \"M\", 3000),\n",
    "(2, \"Rose\", 1, \"2010\", \"20\", \"M\", 4000),\n",
    "(3, \"Williams\", 1, \"2010\", \"10\", \"M\", 1000),\n",
    "(4, \"Jones\", 2, \"2005\", \"10\", \"F\", 2000),\n",
    "(5, \"Brown\", 2, \"2010\", \"40\", \"\", -1),\n",
    "(6, \"Brown\", 2, \"2010\", \"50\", \"\", -1)\n",
    "]\n",
    "empColumns = [\"emp_id\", \"name\", \"superior_emp_id\", \"year_joined\", \\\n",
    "\"emp_dept_id\", \"gender\", \"salary\"]\n",
    "\n",
    "df1 =spark.createDataFrame(data=emp, schema=empColumns)\n",
    "df1.printSchema()\n",
    "df1.show(truncate = False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1c8be4-f2c2-49cb-bf02-8138d19eae46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\", 10),\n",
    "(\"Marketing\", 20),\n",
    "(\"Sales\", 30),\n",
    "(\"IT\", 40)\n",
    "]\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data=dept, schema=deptColumns)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ca5efd-778b-4a38-968b-7f6faa4d898f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#we have created the two dataframe now we have to join those dataframes using different types of joins\n",
    "\n",
    "#1.Inner Join\n",
    "\n",
    "df1.join(df2, df1.emp_dept_id == df2.dept_id).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93f50b0-5a78-4a24-889c-1a60226dfdf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#2.left join or left outer join\n",
    "\n",
    "df1.join(df2, df1.emp_dept_id == df2.dept_id, \"left\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac6c01d-e71d-4856-9701-8e567dbf3c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#.3.right join ot right outer join\n",
    "\n",
    "df1.join(df2, df1.emp_dept_id == df2.dept_id, \"right\").show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f540b94-c977-4840-b48a-c88af0bdce46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#4.full outer join \n",
    " \n",
    "\n",
    "df1.join(df2, df1.emp_dept_id == df2.dept_id,\"outer\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9743ae7f-6607-4ddd-ba90-a884b26b8268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#5. left semi join\n",
    "\n",
    "df1.join(df2, df1.emp_dept_id == df2.dept_id,\"left_semi\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bd2fa7-df41-482f-a372-0a99fef91a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#6.left anti join\n",
    "\n",
    "df1.join(df2, df1.emp_dept_id == df2.dept_id,\"left_anti\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5b3454-3900-463e-aff6-e52561759708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#using sql expressions also we can query the same in pyspark\n",
    "\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "\n",
    "joindf = spark.sql(\"select * from df1, df2 where df1.emp_dept_id  == df2.dept_id\").show(truncate = False)\n",
    "\n",
    "joindf1 = spark.sql(\"select * from df1 inner join df2 on df1.emp_dept_id  == df2.dept_id\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9540ce-adbd-4ba1-8c8a-dc1adc7c765f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+--------------------+\n| id|  name|  deptartment|              skills|\n+---+------+-------------+--------------------+\n|  1| Fiona|    Marketing|Social Media Stra...|\n|  2|George|        Sales|     Lead Generation|\n|  3|Hannah|  Engineering|    Machine Learning|\n|  4|   Ian|Customer Care|      CRM Management|\n|  5| Julia|      Finance|       Risk Analysis|\n+---+------+-------------+--------------------+\n\n+---+----------+\n| id|name_ltrim|\n+---+----------+\n|  1|     Fiona|\n|  2|    George|\n|  3|    Hannah|\n|  4|       Ian|\n|  5|     Julia|\n+---+----------+\n\n+---+----------+\n| id|name_rtrim|\n+---+----------+\n|  1|     Fiona|\n|  2|    George|\n|  3|    Hannah|\n|  4|       Ian|\n|  5|     Julia|\n+---+----------+\n\n+---+---------+\n| id|name_trim|\n+---+---------+\n|  1|    Fiona|\n|  2|   George|\n|  3|   Hannah|\n|  4|      Ian|\n|  5|    Julia|\n+---+---------+\n\n+---+----------+\n| id| name_lpad|\n+---+----------+\n|  1|*****Fiona|\n|  2|****George|\n|  3|****Hannah|\n|  4|*******Ian|\n|  5|*****Julia|\n+---+----------+\n\n+---+-------------+----------+\n| id|  deptartment| name_rpad|\n+---+-------------+----------+\n|  1|    Marketing|Fiona*****|\n|  2|        Sales|George****|\n|  3|  Engineering|Hannah****|\n|  4|Customer Care|Ian*******|\n|  5|      Finance|Julia*****|\n+---+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# trim() function \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import ltrim, rtrim, trim, lpad, rpad\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "emp_data  = [(1, \"Fiona\", \"Marketing\", \"Social Media Strategy\"),\n",
    "(2, \"George\", \"Sales\", \"Lead Generation\"),\n",
    "(3, \"Hannah\", \"Engineering\", \"Machine Learning\"),\n",
    "(4, \"Ian\", \"Customer Care\", \"CRM Management\"),\n",
    "(5, \"Julia\", \"Finance\", \"Risk Analysis\")\n",
    "] \n",
    "\n",
    "columns = [\"id\", \"name\", \"deptartment\", \"skills\"]\n",
    "\n",
    "df = spark.createDataFrame(data = emp_data, schema = columns)\n",
    "\n",
    "#demonstrate  trim function\n",
    "df_trim = df.select(\"id\",ltrim(df[\"name\"]).alias(\"name_ltrim\"))\n",
    "\n",
    "df_rtrim = df.select(\"id\",rtrim(df[\"name\"]).alias(\"name_rtrim\"))\n",
    "\n",
    "df_trimf = df.select(\"id\",trim(df[\"name\"]).alias(\"name_trim\"))\n",
    "\n",
    "df_lpad = df.select(\"id\",lpad(df[\"name\"],10,\"*\").alias(\"name_lpad\"))\n",
    "\n",
    "df_rpad = df.select(\"id\",\"deptartment\",rpad(df[\"name\"],10,\"*\").alias(\"name_rpad\"))\n",
    "\n",
    "df.show()\n",
    "df_trim.show()\n",
    "df_rtrim.show()\n",
    "df_trimf.show()\n",
    "df_lpad.show()\n",
    "df_rpad.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4210ff05-1dd6-46cb-b4aa-f4411b696dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Adding, renaming, dropping columns \n",
    "\n",
    "#1. withcolumn() - it used to add the new column in th dataframe\n",
    "#2. withcolumnRenamed() - it used to rename the existing column\n",
    "#3. drop() - it used to drop the existing column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d64cd6-fc4e-4421-82fa-acc16efb8419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n| id|     today|                 now|\n+---+----------+--------------------+\n|  0|2024-11-25|2024-11-25 09:37:...|\n|  1|2024-11-25|2024-11-25 09:37:...|\n|  2|2024-11-25|2024-11-25 09:37:...|\n|  3|2024-11-25|2024-11-25 09:37:...|\n|  4|2024-11-25|2024-11-25 09:37:...|\n|  5|2024-11-25|2024-11-25 09:37:...|\n|  6|2024-11-25|2024-11-25 09:37:...|\n|  7|2024-11-25|2024-11-25 09:37:...|\n|  8|2024-11-25|2024-11-25 09:37:...|\n|  9|2024-11-25|2024-11-25 09:37:...|\n+---+----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#date functions \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, datediff, months_between\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "df = spark.range(10).withColumn(\"today\",current_date()).withColumn(\"now\",current_timestamp())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed90fc09-f257-4d86-8c2e-a8174ecd4b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|adding_date|\n+-----------+\n| 2024-11-30|\n| 2024-11-30|\n| 2024-11-30|\n| 2024-11-30|\n| 2024-11-30|\n| 2024-11-30|\n| 2024-11-30|\n| 2024-11-30|\n| 2024-11-30|\n| 2024-11-30|\n+-----------+\n\n+----------------+\n|subtracting_date|\n+----------------+\n|      2024-11-23|\n|      2024-11-23|\n|      2024-11-23|\n|      2024-11-23|\n|      2024-11-23|\n|      2024-11-23|\n|      2024-11-23|\n|      2024-11-23|\n|      2024-11-23|\n|      2024-11-23|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#date_add and date_sub\n",
    "\n",
    "df.select(date_add(\"today\",5).alias(\"adding_date\")).show()\n",
    "df.select(date_sub(\"today\",2).alias(\"subtracting_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a7a4f3-3aca-4e58-a4eb-1e97379a5b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n| id|     today|  week_ago|\n+---+----------+----------+\n|  0|2024-11-25|2024-11-18|\n|  1|2024-11-25|2024-11-18|\n|  2|2024-11-25|2024-11-18|\n|  3|2024-11-25|2024-11-18|\n|  4|2024-11-25|2024-11-18|\n|  5|2024-11-25|2024-11-18|\n|  6|2024-11-25|2024-11-18|\n|  7|2024-11-25|2024-11-18|\n|  8|2024-11-25|2024-11-18|\n|  9|2024-11-25|2024-11-18|\n+---+----------+----------+\n\n+---------+\n|diff_date|\n+---------+\n|        7|\n|        7|\n|        7|\n|        7|\n|        7|\n|        7|\n|        7|\n|        7|\n|        7|\n|        7|\n+---------+\n\n+----------+\n|diff_month|\n+----------+\n|0.22580645|\n|0.22580645|\n|0.22580645|\n|0.22580645|\n|0.22580645|\n|0.22580645|\n|0.22580645|\n|0.22580645|\n|0.22580645|\n|0.22580645|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#date_diff and months_diff\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, datediff, months_between\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "df = spark.range(10).withColumn(\"today\",current_date()).withColumn(\"week_ago\",date_sub(\"today\",7))\n",
    "df.show()\n",
    "\n",
    "df.select(datediff(\"today\",\"week_ago\").alias(\"diff_date\")).show()\n",
    "df.select(months_between(\"today\",\"week_ago\").alias(\"diff_month\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63bed51-99d5-4d59-8282-9432d412ef7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original Dataframe\n+---------+-------------+---------+---------+\n|  country|       region|UnitsSold|UnitPrice|\n+---------+-------------+---------+---------+\n|      USA|North America|      100|     50.5|\n|    India|         Asia|      300|     20.0|\n|  Germany|       Europe|      200|     30.5|\n|Australia|      Oceania|      150|     60.0|\n|    Japan|         Asia|      120|     45.0|\n|   Brazil|South America|      180|     25.0|\n+---------+-------------+---------+---------+\n\nunitsold by descending order\n+---------+-------------+---------+---------+\n|  country|       region|UnitsSold|UnitPrice|\n+---------+-------------+---------+---------+\n|    India|         Asia|      300|     20.0|\n|  Germany|       Europe|      200|     30.5|\n|   Brazil|South America|      180|     25.0|\n|Australia|      Oceania|      150|     60.0|\n|    Japan|         Asia|      120|     45.0|\n|      USA|North America|      100|     50.5|\n+---------+-------------+---------+---------+\n\nregion by ascending order\n+---------+-------------+---------+---------+\n|  country|       region|UnitsSold|UnitPrice|\n+---------+-------------+---------+---------+\n|    Japan|         Asia|      120|     45.0|\n|    India|         Asia|      300|     20.0|\n|  Germany|       Europe|      200|     30.5|\n|      USA|North America|      100|     50.5|\n|Australia|      Oceania|      150|     60.0|\n|   Brazil|South America|      180|     25.0|\n+---------+-------------+---------+---------+\n\nconcate two columns country and region\n+---------+-------------+---------+---------+\n|  country|       region|UnitsSold|UnitPrice|\n+---------+-------------+---------+---------+\n|      USA|North America|      100|     50.5|\n|    India|         Asia|      300|     20.0|\n|  Germany|       Europe|      200|     30.5|\n|Australia|      Oceania|      150|     60.0|\n|    Japan|         Asia|      120|     45.0|\n|   Brazil|South America|      180|     25.0|\n+---------+-------------+---------+---------+\n\nusing upper, lower, initcap\n+---------+-------------+---------+---------+-----------+-------------+-------------+\n|  country|       region|UnitsSold|UnitPrice|country_upp|   region_low|  region_init|\n+---------+-------------+---------+---------+-----------+-------------+-------------+\n|      USA|North America|      100|     50.5|        USA|north america|North America|\n|    India|         Asia|      300|     20.0|      INDIA|         asia|         Asia|\n|  Germany|       Europe|      200|     30.5|    GERMANY|       europe|       Europe|\n|Australia|      Oceania|      150|     60.0|  AUSTRALIA|      oceania|      Oceania|\n|    Japan|         Asia|      120|     45.0|      JAPAN|         asia|         Asia|\n|   Brazil|South America|      180|     25.0|     BRAZIL|south america|South America|\n+---------+-------------+---------+---------+-----------+-------------+-------------+\n\nif the Oceania in region checking\n+---------+-------+---------+---------+-----------+----------+-----------+\n|  country| region|UnitsSold|UnitPrice|country_upp|region_low|region_init|\n+---------+-------+---------+---------+-----------+----------+-----------+\n|Australia|Oceania|      150|     60.0|  AUSTRALIA|   oceania|    Oceania|\n+---------+-------+---------+---------+-----------+----------+-----------+\n\nlength of the country\n+---------+-------------+---------+---------+-----------+-------------+-------------+-----------+\n|  country|       region|UnitsSold|UnitPrice|country_upp|   region_low|  region_init|country_len|\n+---------+-------------+---------+---------+-----------+-------------+-------------+-----------+\n|      USA|North America|      100|     50.5|        USA|north america|North America|          3|\n|    India|         Asia|      300|     20.0|      INDIA|         asia|         Asia|          5|\n|  Germany|       Europe|      200|     30.5|    GERMANY|       europe|       Europe|          7|\n|Australia|      Oceania|      150|     60.0|  AUSTRALIA|      oceania|      Oceania|          9|\n|    Japan|         Asia|      120|     45.0|      JAPAN|         asia|         Asia|          5|\n|   Brazil|South America|      180|     25.0|     BRAZIL|south america|South America|          6|\n+---------+-------------+---------+---------+-----------+-------------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Data ordering\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc, asc, concat, concat_ws, initcap, lower, upper, instr,\\\n",
    "  length, lit\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "(\"USA\", \"North America\", 100, 50.5),\n",
    "(\"India\", \"Asia\", 300, 20.0),\n",
    "(\"Germany\", \"Europe\", 200, 30.5),\n",
    "(\"Australia\", \"Oceania\", 150, 60.0),\n",
    "(\"Japan\", \"Asia\", 120, 45.0),\n",
    "(\"Brazil\", \"South America\", 180, 25.0)\n",
    "]\n",
    "\n",
    "column = [\"country\", \"region\", \"UnitsSold\", \"UnitPrice\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=column)\n",
    "print(\"original Dataframe\")\n",
    "df.show()\n",
    "\n",
    "print(\"unitsold by descending order\")\n",
    "df.sort(desc(\"UnitsSold\")).show()\n",
    "\n",
    "print(\"region by ascending order\")\n",
    "df.sort(asc(\"region\")).show()\n",
    "\n",
    "#strings - concatenate two columns  country and region\n",
    "print(\"concate two columns country and region\")\n",
    "df.withColumn(\"country&region\", concat(col(\"country\"), lit(\" \"), col(\"region\")))\n",
    "df.show()\n",
    "\n",
    "print(\"using upper, lower, initcap\")\n",
    "df= df.withColumn(\"country_upp\",upper(col(\"country\")))\\\n",
    "   .withColumn(\"region_low\",lower(col(\"region\")))\\\n",
    "    .withColumn(\"region_init\",initcap(col(\"region\")))\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(\"if the Oceania in region checking\")\n",
    "\n",
    "df.filter(instr(col(\"region\"),\"Oceania\")>0).show()\n",
    "\n",
    "print(\"length of the country\")\n",
    "df.withColumn(\"country_len\",length(col(\"country\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a397b9b2-d787-4484-9af2-a246f0430b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- price: long (nullable = true)\n |-- phone: long (nullable = true)\n\n+---+-----+---------+\n| id|price|    phone|\n+---+-----+---------+\n|  1| 2000|930303099|\n|  2| 3000| 89020190|\n|  3| 4000| 23091030|\n+---+-----+---------+\n\n+---+-----+---------+---------+\n| id|price|    phone|price_new|\n+---+-----+---------+---------+\n|  1| 2000|930303099|   2000.0|\n|  2| 3000| 89020190|   3000.0|\n|  3| 4000| 23091030|   4000.0|\n+---+-----+---------+---------+\n\n+---+-----+---------+---------+\n| id|price|    phone|phone_new|\n+---+-----+---------+---------+\n|  1| 2000|930303099|930303099|\n|  2| 3000| 89020190| 89020190|\n|  3| 4000| 23091030| 23091030|\n+---+-----+---------+---------+\n\n+---+-----+--------+\n| id|price|   phone|\n+---+-----+--------+\n|  3| 4000|23091030|\n+---+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#Dataframe Filter\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "data = [(1,2000,930303099),(2,3000,89020190),(3,4000,23091030)]\n",
    "column = [\"id\",\"price\",\"phone\"]\n",
    "df = spark.createDataFrame(data=data, schema=column)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df.withColumn(\"price_new\",col(\"price\").cast(\"double\")).show()\n",
    "df.withColumn(\"phone_new\", col(\"phone\").cast(\"string\")).show()\n",
    "\n",
    "df.filter(col(\"price\")>3000).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca6f4bb-904f-4179-89bd-77fc53871f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+-------+\n|Names|Region|UnitSold|Revenue|\n+-----+------+--------+-------+\n| John| North|     100|   NULL|\n|  Doe|  East|    NULL|     50|\n| NULL|  West|     150|     30|\n|Alice|  NULL|     200|     40|\n|  Bob| South|    NULL|   NULL|\n| NULL|  NULL|    NULL|   NULL|\n+-----+------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Null Handling\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "# Sample data: sales data with nulls\n",
    "data = [\n",
    "(\"John\", \"North\", 100, None),\n",
    "(\"Doe\", \"East\", None, 50),\n",
    "(None, \"West\", 150, 30),\n",
    "(\"Alice\", None, 200, 40),\n",
    "(\"Bob\", \"South\", None, None),\n",
    "(None, None, None, None)\n",
    "]\n",
    "\n",
    "column = [\"Names\",\"Region\",\"UnitSold\",\"Revenue\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=column)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e3d968-f834-445b-bfd6-f397b0ef1555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+-------+----------+\n|Names|Region|UnitSold|Revenue|new_region|\n+-----+------+--------+-------+----------+\n| John| North|     100|   NULL|     false|\n|  Doe|  East|    NULL|     50|     false|\n| NULL|  West|     150|     30|     false|\n|Alice|  NULL|     200|     40|      true|\n|  Bob| South|    NULL|   NULL|     false|\n| NULL|  NULL|    NULL|   NULL|      true|\n+-----+------+--------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#1.isnull()\n",
    "from pyspark.sql.functions import *\n",
    "df1 = df.withColumn(\"new_region\" , isnull(col(\"Region\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d16196f-3179-4026-b1ca-899077265328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+-------+\n|Names|Region|UnitSold|Revenue|\n+-----+------+--------+-------+\n+-----+------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#2.dropna()\n",
    "df2 = df.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5edf20-f6c8-445a-883e-e32dba504a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+-------+\n|Names|Region|UnitSold|Revenue|\n+-----+------+--------+-------+\n| John| North|     100|   NULL|\n|  Doe|  East|    NULL|     50|\n| NULL|  West|     150|     30|\n|Alice|  NULL|     200|     40|\n|  Bob| South|    NULL|   NULL|\n+-----+------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#3.dropna()\n",
    "df3 = df.dropna(\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63de53c-67db-42e6-a36b-ebaea692e24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+-------+\n|Names|Region|UnitSold|Revenue|\n+-----+------+--------+-------+\n| John| North|     100|   NULL|\n|  Doe|  East|    NULL|     50|\n| NULL|  West|     150|     30|\n|Alice|  NULL|     200|     40|\n+-----+------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#4.dropna()\n",
    "df4 = df.dropna(\"all\",subset=[\"UnitSold\",\"Revenue\"])\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b459f81c-dd75-472b-b505-2373fdfd38d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+-------+\n|  Names|Region|UnitSold|Revenue|\n+-------+------+--------+-------+\n|   John| North|     100|      0|\n|    Doe|  East|       0|     50|\n|Unknown|  West|     150|     30|\n|  Alice|  ....|     200|     40|\n|    Bob| South|       0|      0|\n|Unknown|  ....|       0|      0|\n+-------+------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#5.fillna()\n",
    "\n",
    "df5 = df.fillna({\"Names\":\"Unknown\",\"Region\":\"....\",\"UnitSold\":\"0\",\"Revenue\":\"0\"})\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a9d652-7ae5-4ba1-94ce-492885397a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+-------+---------------+\n|Names|Region|UnitSold|Revenue|coalesc_revenue|\n+-----+------+--------+-------+---------------+\n| John| North|     100|   NULL|            100|\n|  Doe|  East|    NULL|     50|             50|\n| NULL|  West|     150|     30|             30|\n|Alice|  NULL|     200|     40|             40|\n|  Bob| South|    NULL|   NULL|           NULL|\n| NULL|  NULL|    NULL|   NULL|           NULL|\n+-----+------+--------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#6.coalesc()\n",
    "df6 = df.withColumn(\"coalesc_revenue\",coalesce(\"Revenue\",\"UnitSold\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ec2b01-260d-4ce3-81b2-bad9e44a2c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n|Region|avg_unitsold|\n+------+------------+\n| North|       100.0|\n|  East|         0.0|\n|  West|       150.0|\n|  NULL|       200.0|\n| South|         0.0|\n+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Handling nulls in aggregation\n",
    "df7 = df.groupBy(\"Region\").agg(coalesce(mean(\"UnitSold\"),lit(0)).alias(\"avg_unitsold\"))\n",
    "df7.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Pyspark - interview Series Practice Collection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}