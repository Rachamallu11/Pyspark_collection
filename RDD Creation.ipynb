{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa629913-5889-4172-baae-609f4c60cdb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RDD - Resilient Distributed Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6602f9ee-39a8-4c05-981f-4d1a6e3ef493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
     ]
    }
   ],
   "source": [
    "#creation if RDD\n",
    "#1.here we are create the RDD using parallel collection\n",
    "from pyspark.sql import SparkSession\n",
    "spark  = SparkSession.builder.appName(\"RDD\").getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(range(1,11)) # if we define the range it will directly create the upto our range of numbers given\n",
    "rdd.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6359e35e-d242-4e9e-bb2b-5bd8333ba6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: [1, 10]"
     ]
    }
   ],
   "source": [
    "#for list\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1,10])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45af0048-adfe-45d2-913d-fc7f18d2d11d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [(1, 'satya'), (2, 'rana')]"
     ]
    }
   ],
   "source": [
    "#for tuple\n",
    "#'sc' means spark.sparkContext\n",
    "data = [(1,\"satya\"),(2,\"rana\")]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e50946-d388-464f-8d6c-85f9d87c174a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [{1: 'satya', 2: 'rana', 3: 'ro'}]"
     ]
    }
   ],
   "source": [
    "#for dictionary\n",
    "\n",
    "data = {1:'satya',2:'rana',3:'ro'}\n",
    "rdd = sc.parallelize([data])  #why we are using [] means it we not mentioned then it will return only key's at output.\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3d84fcf-4ba3-4f42-a951-8c26bd036614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.Here creation of RDD using External Data Sources\n",
    "#here if we have a file in ADLS GEN2 location then we can take the file from external data source below we have to copy the file path and paste and print here\n",
    "rdd_extds = sc.textFile(\"file path\")\n",
    "rdd.extds.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c4d6ec-adae-4523-8b86-ee23016733e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: [30, 20, 12]"
     ]
    }
   ],
   "source": [
    "#3. we can create the RDD from the exsisting RDD\n",
    "#here we are created the new rdd using exsisting RDD\n",
    "data = [15,10,6]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()\n",
    "newrdd = rdd.map(lambda x:x*2)\n",
    "newrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacf546e-c064-4a83-b781-ea5fe4b4e5f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n| no|name|\n+---+----+\n|  1| rdd|\n|  2| set|\n+---+----+\n\nOut[11]: [Row(no=1, name='rdd'), Row(no=2, name='set')]"
     ]
    }
   ],
   "source": [
    "#4.we can create RDD from DataFrame & Data Sets\n",
    "\n",
    "data = [(1,\"rdd\"),(2,\"set\")]\n",
    "schema = [\"no\",\"name\"]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "rdd = df.rdd\n",
    "rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RDD Creation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
