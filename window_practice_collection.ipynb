{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e831a12-10b9-4289-abe9-2f866e77968b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n|   name|      date|sales|\n+-------+----------+-----+\n|  Alice|2023-01-01|  500|\n|  Alice|2023-01-02|  600|\n|  Alice|2023-01-03|  700|\n|    Bob|2023-01-01|  400|\n|    Bob|2023-01-02|  300|\n|    Bob|2023-01-03|  200|\n|Charlie|2023-01-01|  700|\n|Charlie|2023-01-02|  800|\n|Charlie|2023-01-03|  900|\n+-------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#here we created the dataframe now we have to do row_num, rank, runningtotal\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number,rank, sum as spark_sum,dense_rank\n",
    "\n",
    "spark = SparkSession.builder.appName(\"window example\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    ('Alice', '2023-01-01', 500),\n",
    "    ('Alice', '2023-01-02', 600),\n",
    "    ('Alice', '2023-01-03', 700),\n",
    "    ('Bob', '2023-01-01', 400),\n",
    "    ('Bob', '2023-01-02', 300),\n",
    "    ('Bob', '2023-01-03', 200),\n",
    "    ('Charlie', '2023-01-01', 700),\n",
    "    ('Charlie', '2023-01-02', 800),\n",
    "    ('Charlie', '2023-01-03', 900)\n",
    "]\n",
    "\n",
    "columns = ['name', 'date', 'sales']\n",
    "\n",
    "df = spark.createDataFrame(data = data,schema = columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99c5452-04cd-442a-853a-b8b6bfdf329d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+----------+\n|   name|      date|sales|row_number|\n+-------+----------+-----+----------+\n|  Alice|2023-01-01|  500|         1|\n|  Alice|2023-01-02|  600|         2|\n|  Alice|2023-01-03|  700|         3|\n|    Bob|2023-01-01|  400|         1|\n|    Bob|2023-01-02|  300|         2|\n|    Bob|2023-01-03|  200|         3|\n|Charlie|2023-01-01|  700|         1|\n|Charlie|2023-01-02|  800|         2|\n|Charlie|2023-01-03|  900|         3|\n+-------+----------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#1.ro_num\n",
    "\n",
    "window_spec = Window.partitionBy(\"name\").orderBy(\"date\")\n",
    "df_row_num = df.withColumn(\"row_number\", row_number().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3416053b-8088-44a1-b081-eab5a04f3666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+----+\n|   name|      date|sales|rank|\n+-------+----------+-----+----+\n|  Alice|2023-01-01|  500|   1|\n|  Alice|2023-01-02|  600|   2|\n|  Alice|2023-01-03|  700|   3|\n|    Bob|2023-01-01|  400|   1|\n|    Bob|2023-01-02|  300|   2|\n|    Bob|2023-01-03|  200|   3|\n|Charlie|2023-01-01|  700|   1|\n|Charlie|2023-01-02|  800|   2|\n|Charlie|2023-01-03|  900|   3|\n+-------+----------+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "#2.rank\n",
    "df_rank = df.withColumn(\"rank\",rank().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bdf68b6-c414-483d-a10a-1b171a9f3a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+-------------+\n|   name|      date|sales|running_total|\n+-------+----------+-----+-------------+\n|  Alice|2023-01-01|  500|          500|\n|  Alice|2023-01-02|  600|         1100|\n|  Alice|2023-01-03|  700|         1800|\n|    Bob|2023-01-01|  400|          400|\n|    Bob|2023-01-02|  300|          700|\n|    Bob|2023-01-03|  200|          900|\n|Charlie|2023-01-01|  700|          700|\n|Charlie|2023-01-02|  800|         1500|\n|Charlie|2023-01-03|  900|         2400|\n+-------+----------+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#3.running_total\n",
    "\n",
    "df_running_total = df.withColumn(\"running_total\", spark_sum(\"sales\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355cfe44-ea14-4931-9983-4aca7facd3e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+--------------+------------------+\n|   name|      date|sales|next_day_sales|previous_day_sales|\n+-------+----------+-----+--------------+------------------+\n|  Alice|2023-01-01|  500|           600|              null|\n|  Alice|2023-01-02|  600|           700|               500|\n|  Alice|2023-01-03|  700|          null|               600|\n|    Bob|2023-01-01|  400|           300|              null|\n|    Bob|2023-01-02|  300|           200|               400|\n|    Bob|2023-01-03|  200|          null|               300|\n|Charlie|2023-01-01|  700|           800|              null|\n|Charlie|2023-01-02|  800|           900|               700|\n|Charlie|2023-01-03|  900|          null|               800|\n+-------+----------+-----+--------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#4.lead and lag\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lead, lag\n",
    "\n",
    "spark = SparkSession.builder.appName(\"lead and lag function\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    ('Alice', '2023-01-01', 500),\n",
    "    ('Alice', '2023-01-02', 600),\n",
    "    ('Alice', '2023-01-03', 700),\n",
    "    ('Bob', '2023-01-01', 400),\n",
    "    ('Bob', '2023-01-02', 300),\n",
    "    ('Bob', '2023-01-03', 200),\n",
    "    ('Charlie', '2023-01-01', 700),\n",
    "    ('Charlie', '2023-01-02', 800),\n",
    "    ('Charlie', '2023-01-03', 900)\n",
    "]\n",
    "\n",
    "columns = ['name', 'date', 'sales']\n",
    "\n",
    "df = spark.createDataFrame(data = data,schema = columns)\n",
    "\n",
    "#windowspec\n",
    "window_spec = Window.partitionBy(\"name\").orderBy(\"date\")\n",
    "\n",
    "#lead and lag\n",
    "df_with_lead_lag = df \\\n",
    "    .withColumn(\"next_day_sales\", lead(\"sales\", 1).over(window_spec)) \\\n",
    "    .withColumn(\"previous_day_sales\", lag(\"sales\", 1).over(window_spec))\n",
    "\n",
    "df_with_lead_lag.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e331896-f683-4843-ae67-1ccdc52edb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. interview questions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "window_practice_collection",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
