{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8c3e5a-221e-4e47-a644-c8a7bd783cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+--------------+\n|department_id| Name|departmentname|\n+-------------+-----+--------------+\n|            1| John|   Engineering|\n|            1|  Bob|   Engineering|\n|            2|Alice|         Sales|\n+-------------+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Q.1 \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create SparkSession\n",
    "spark = SparkSession.builder.appName(\"InnerJoinExample\").getOrCreate()\n",
    "\n",
    "#Data\n",
    "data1 = [(\"John\",1),(\"Alice\",2),(\"Bob\",1)]\n",
    "\n",
    "data2 =[(1,\"Engineering\"),(2,\"Sales\")]\n",
    "\n",
    "#Create DataFrames\n",
    "\n",
    "df1 = spark.createDataFrame(data1,[\"Name\",\"department_id\"])\n",
    "df2 = spark.createDataFrame(data2,[\"department_id\",\"departmentname\"])\n",
    "\n",
    "#perform inner join\n",
    "inner_join = df1.join(df2,on=\"department_id\",how=\"inner\")\n",
    "\n",
    "inner_join.show()   #it will print the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412cb2ec-eb73-4f90-aa1d-34d5879a42a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------------+\n|department_id| Name|department_name|\n+-------------+-----+---------------+\n|            1| John|    Engineering|\n|            2|Alice|           null|\n|            1|  Bob|    Engineering|\n+-------------+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Q.2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Left join\").getOrCreate()\n",
    "\n",
    "d1 = [(\"John\",1),(\"Alice\",2),(\"Bob\",1)]\n",
    "\n",
    "d2 = [(1,\"Engineering\"),(3,\"Marketing\")]\n",
    "\n",
    "df1 = spark.createDataFrame(d1,[\"Name\",\"department_id\"])\n",
    "df2 =  spark.createDataFrame(d2, [\"department_id\",\"department_name\"])\n",
    "\n",
    "left_join = df1.join(df2,on=\"department_id\",how=\"left\")\n",
    "\n",
    "left_join.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc23d1e-d0ea-43c4-949f-a158901927d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+---------------+\n|department_id|name|department_name|\n+-------------+----+---------------+\n|            1|john|    Engineering|\n|            3|null|      Marketing|\n+-------------+----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Q.3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"right\").getOrCreate()\n",
    "\n",
    "d1 =[(\"john\",1),(\"alice\",2),(\"bob\",2)]\n",
    "\n",
    "d2 = [(1,\"Engineering\"),(3,\"Marketing\")]\n",
    "\n",
    "df1 = spark.createDataFrame(d1,[\"name\",\"department_id\"])\n",
    "df2 = spark.createDataFrame(d2,[\"department_id\",\"department_name\"])\n",
    "\n",
    "right_join = df1.join(df2,on = \"department_id\", how=\"right\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f6cf4c7-70aa-4fa0-a195-3966ca1d2354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------------+\n|department_id| name|department_name|\n+-------------+-----+---------------+\n|            1| john|    Engineering|\n|            2|alice|           null|\n|            2|  bob|           null|\n|            3| null|      Marketing|\n+-------------+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Q.4\n",
    "\n",
    "full_join = df1.join(df2,on=\"department_id\",how=\"full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013d18fe-6fd5-4757-941b-e0e0f6e453f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n|department_id|name|\n+-------------+----+\n|            1|john|\n+-------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "#Q.5\n",
    "left_semi_join = df1.join(df2,on=\"department_id\",how = \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf9ed55-1f3b-4a17-b43d-bcae46984aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n|department_id| name|\n+-------------+-----+\n|            2|alice|\n|            2|  bob|\n+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Q.6\n",
    "\n",
    "left_anti_join = df1.join(df2,on = \"department_id\", how = \"left_anti\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark-Joins-Practice Collection",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
