{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390e870f-669e-4b29-a76d-35dd134f6659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  2|rohitha| 20|\n|  1|  satya| 24|\n|  3|   rana| 30|\n+---+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#1.Creating Delta Table and inserting the data into it.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaTable\").getOrCreate()\n",
    "\n",
    "data = [(1, \"satya\",24),(2,\"rohitha\",20),(3, \"rana\",30)]\n",
    "columns = [\"id\",\"name\",\"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "\n",
    "#converting dataframe into deltatable\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/people\")\n",
    "\n",
    "#check the delta table content\n",
    "delta_df = spark.read.format(\"delta\").load(\"/delta/people\")\n",
    "delta_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6827c2a-1f38-4151-83b9-1f599f1c7478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+\n|  id|   name|age|\n+----+-------+---+\n|   2|rohitha| 20|\n|   1|  satya| 25|\n|   3|   rana| 30|\n|NULL|  shyam| 40|\n|NULL|  mouni| 30|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n+----+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#2.upsert data using mergeinto- adding columns and updating existing columns\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "#create exsisting deltatable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/delta/people\")\n",
    "\n",
    "#add new data\n",
    "new_data = [(1,\"satya\",25),(4,\"mouni\",30)]\n",
    "columns = [\"id\",\"name\",\"age\"]\n",
    "new_df = spark.createDataFrame(new_data,columns)\n",
    "\n",
    "#create the temp view for new data\n",
    "new_df.createOrReplaceTempView(\"new_data\")\n",
    "\n",
    "#\n",
    "delta_table.alias(\"target\").merge(\n",
    "      source = spark.table(\"new_data\").alias(\"source\"),\n",
    "      condition = \"target.id = source.id\"\n",
    ").whenMatchedUpdate(set ={\"age\":\"source.age\"})\\\n",
    ".whenNotMatchedInsert(values={\"name\":\"source.name\", \"age\":\"source.age\"}).execute()\n",
    "\n",
    "#verify\n",
    "delta_table.toDF().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bf30c0-0f48-4954-b03f-b204c8236e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+\n|  id|   name|age|\n+----+-------+---+\n|   2|rohitha| 20|\n|   1|  satya| 25|\n|   3|   rana| 30|\n|NULL|  shyam| 40|\n|NULL|  mouni| 30|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n+----+-------+---+\n\n+----+-------+---+\n|  id|   name|age|\n+----+-------+---+\n|   2|rohitha| 20|\n|   1|  satya| 25|\n|   3|   rana| 30|\n|NULL|    ram| 35|\n+----+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#3.implementing time travel in delta tables\n",
    "\n",
    "#display the current data and version of delta table\n",
    "current_df = spark.read.format(\"delta\").load(\"/delta/people\")\n",
    "current_df.show()\n",
    "\n",
    "#query a previous version of the delta table\n",
    "version_0_df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/delta/people\")\n",
    "version_0_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363d23e6-daa4-4c52-91e1-ac423fe561d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+\n|  id|   name|age|\n+----+-------+---+\n|   2|rohitha| 20|\n|   1|  satya| 26|\n|   3|   rana| 30|\n|NULL|  mouni| 31|\n|NULL|  shyam| 40|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n+----+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#here we are again adding new data (mergeinto) and checking the version using timetravel\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "#create exsisting deltatable\n",
    "delta_table = DeltaTable.forPath(spark,\"/delta/people\")\n",
    "\n",
    "#add new data \n",
    "new_data1 = [(1,\"satya\",26),(6,\"mouni\",31)]\n",
    "columns = [\"id\",\"name\",\"age\"]\n",
    "new_df1 = spark.createDataFrame(new_data1,columns)\n",
    "\n",
    "#create view for new data\n",
    "new_df1.createOrReplaceTempView(\"new_data1\")\n",
    "\n",
    "#mergeinto\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source=spark.table(\"new_data1\").alias(\"source\"),\n",
    "    condition=\"target.name = source.name\"\n",
    ").whenMatchedUpdate(set={\"age\": \"source.age\"}) \\\n",
    " .whenNotMatchedInsert(values={\"name\": \"source.name\", \"age\": \"source.age\"}) \\\n",
    " .execute()\n",
    "\n",
    "#check\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "332ddb8c-2620-4019-8a36-5bc1a1222af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+\n|  id|   name|age|\n+----+-------+---+\n|   2|rohitha| 20|\n|   1|  satya| 26|\n|   3|   rana| 30|\n|NULL|  mouni| 31|\n|NULL|  shyam| 40|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n+----+-------+---+\n\n+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  2|rohitha| 20|\n|  1|  satya| 24|\n|  3|   rana| 30|\n+---+-------+---+\n\n+----+-------+---+\n|  id|   name|age|\n+----+-------+---+\n|   2|rohitha| 20|\n|   1|  satya| 25|\n|   3|   rana| 30|\n|NULL|    ram| 35|\n+----+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#check with timetravel versions\n",
    "\n",
    "#current data version\n",
    "current_df = spark.read.format(\"delta\").load(\"/delta/people\")\n",
    "current_df.show()\n",
    "\n",
    "#check the time travel query - for 0\n",
    "version_0_df =spark.read.format(\"delta\").option(\"VersionAsOf\",0).load(\"/delta/people\")\n",
    "version_0_df.show()\n",
    "\n",
    "#check for version 1\n",
    "version_1_df =spark.read.format(\"delta\").option(\"VersionAsOf\",1).load(\"/delta/people\")\n",
    "version_1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e681812b-9fec-4042-933f-ca0f5520c5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+\n|  id|   name|age|\n+----+-------+---+\n|   2|rohitha| 20|\n|   1|  satya| 26|\n|   3|   rana| 32|\n|NULL|  mouni| 31|\n|NULL|  shyam| 40|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n|NULL|    ram| 35|\n+----+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "##here we are again adding new data (mergeinto) and checking the version using timetravel\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark,\"/delta/people\")\n",
    "\n",
    "new_data2 = [(3,\"rana\",32)]\n",
    "column = [\"id\",\"name\",\"age\"]\n",
    "new_data2_df = spark.createDataFrame(new_data2,column)\n",
    "\n",
    "new_data2_df.createOrReplaceTempView(\"new_data2\")\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source=spark.table(\"new_data2\").alias(\"source\"),\n",
    "    condition=\"target.name = source.name\"\n",
    ").whenMatchedUpdate(set={\"age\": \"source.age\"}) \\\n",
    " .whenNotMatchedInsert(values={\"name\": \"source.name\", \"age\": \"source.age\"}) \\\n",
    " .execute()\n",
    "\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa336295-85f1-454c-a482-5d4123abb36c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating version of the data using time travel\n",
    "\n",
    "#current data\n",
    "current_df = spark.read.format(\"delta\").load(\"/delta/people\")\n",
    "current_df.show()\n",
    "\n",
    "version_0_df = spark.read.format(\"delta\").option(\"VersionAsOf\",0).load(\"/delta/people\")\n",
    "version_0_df.show()\n",
    "\n",
    "version_1_df = spark.read.format(\"delta\").option(\"VersionAsOf\",1).load(\"/delta/people\")\n",
    "version_1_df.show()\n",
    "\n",
    "version_2_df = spark.read.format(\"delta\").option(\"VersionAsOf\",2).load(\"/delta/people\")\n",
    "version_2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51936012-d071-4dca-b2c4-6229d0f91c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n\n+---+----+----+\n| id|name| age|\n+---+----+----+\n|  1|john|  32|\n|  1|john|NULL|\n+---+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "#Using Delta table to Handle schema evolution\n",
    "\n",
    "#creating original dataframe with the two columns\n",
    "initialdata = [(1,\"john\")]\n",
    "initialdata_df = spark.createDataFrame(initialdata,[\"id\",\"name\"])\n",
    "initialdata_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/schemaEvolution\")\n",
    "\n",
    "#create new the dataframe  with new added column\n",
    "new_data = [(1,\"john\",32)]\n",
    "columns = [\"id\",\"name\",\"age\"]\n",
    "new_data_df = spark.createDataFrame(new_data,columns)\n",
    "\n",
    "#append new data with schema evolution\n",
    "new_data_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/delta/schemaEvolution\")\n",
    "\n",
    "#verify the schema evolution\n",
    "schema_df = spark.read.format(\"delta\").load(\"/delta/schemaEvolution\")\n",
    "schema_df.printSchema()\n",
    "schema_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1ead4f-7481-4eb4-9bc3-93dc73a03bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n|  id| name|age|\n+----+-----+---+\n|   3| rana| 32|\n|NULL|mouni| 31|\n|NULL|shyam| 40|\n|NULL|  ram| 35|\n|NULL|  ram| 35|\n|NULL|  ram| 35|\n+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#deleting records from delta table\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table= DeltaTable.forPath(spark,\"/delta/people\")\n",
    "\n",
    "#deleting the data where the age is less than 30\n",
    "delta_table.delete(\"age <30\")\n",
    "\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1935bfac-91f0-4fed-a31f-85a254ccc74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimise Delta table (compact small file)\n",
    "spark.sql(\"OPTIMIZE '/delta/people'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7c18ca-cee9-446b-b7c6-2f78dbb61026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n|  ram|\n|  ram|\n|  ram|\n|shyam|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#filter table with the specific columns\n",
    "\n",
    "filter_df = spark.read.format(\"delta\").load(\"/delta/people\").select(\"name\").filter(\"age >=35\")\n",
    "\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941ed251-e5b6-4353-af6c-989d3b7cfb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n|  id| name|age|\n+----+-----+---+\n|NULL|  ram| 35|\n|NULL|  ram| 35|\n|NULL|  ram| 35|\n|NULL|shyam| 40|\n|NULL|mouni| 31|\n|   3| rana| 32|\n+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#Z- ordered column \n",
    "# Z-Ordering by a column (e.g., \"age\")\n",
    "\n",
    "spark.sql(\"OPTIMIZE  '/delta/people' ZORDER BY (age)\")\n",
    "delta_table.toDF().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Delta tables practice collection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
